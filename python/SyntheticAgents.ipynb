{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Bandit(object):\n",
    "    def __init__(self, n_arms, initial_mu=None, arms_sigma=1):\n",
    "        \n",
    "        # if not specified, each arm is a gaussian with mean randomly sampled from ~N(0, 1)\n",
    "        self.bandits_mu = initial_mu if initial_mu is not None else stats.norm(0, 1).rvs(n_arms)\n",
    "        \n",
    "        # remembers the best arm and its reward\n",
    "        self.best_arm = np.argmax(self.bandits_mu)\n",
    "        self.best_reward = np.max(self.bandits_mu)\n",
    "        \n",
    "        # instantiate the gaussian arms with predefined mu\n",
    "        self.arms = [stats.norm(mu, arms_sigma) for mu in self.bandits_mu]\n",
    "        \n",
    "    def play(self, arm):\n",
    "        \"\"\"\n",
    "        Returns the reward of playing a given arm\n",
    "        \"\"\"\n",
    "        return self.arms[arm].rvs(1)[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class Experiment(object):\n",
    "    def __init__(self, env, agent):\n",
    "        np.random.seed()\n",
    "        self.env = env\n",
    "        self.agent = agent\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.cumulative_reward = 0\n",
    "        self.cumulative_regret = 0      # actual cumulative regret\n",
    "        self.cumulative_regret_exp = 0  # expected cumulative regret\n",
    "        self.freq_best_action = 0       # no. of times the agent select the best arm\n",
    "        \n",
    "        \n",
    "    def run(self, trials):\n",
    "        times_best_action = 0\n",
    "        \n",
    "        for i in range(trials):\n",
    "            # retrieves an action, makes the player learn and store stats\n",
    "            action = self.agent.act()\n",
    "            self.actions.append(action)\n",
    "\n",
    "            reward = self.env.play(action)\n",
    "            self.agent.learn(action, reward)\n",
    "            \n",
    "            # registers statistics\n",
    "            self.rewards.append(reward)\n",
    "            self.cumulative_reward += reward\n",
    "            self.cumulative_regret += self.env.best_reward - reward\n",
    "            self.cumulative_regret_exp += self.env.best_reward - self.env.bandits_mu[action]\n",
    "            \n",
    "            if action == self.env.best_arm:\n",
    "                times_best_action += 1\n",
    "                \n",
    "            #print('%d, %d, %f' % (i, action, reward))\n",
    "                \n",
    "        self.freq_best_action = times_best_action / trials\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "import copy\n",
    "\n",
    "class RepeatedExperiment(object):\n",
    "    def __init__(self, env, agent, repetitions=5):\n",
    "        self.experiments = [Experiment(env, agent) for _ in range(repetitions)]\n",
    "        self.result = None\n",
    "        \n",
    "    def run_binded(self, experiment):\n",
    "        experiment.run(self.trials)\n",
    "        return copy.copy(experiment)\n",
    "        \n",
    "    def run(self, trials):\n",
    "        self.trials = trials\n",
    "\n",
    "        #p = multiprocessing.Pool(max(10, len(self.experiments)))\n",
    "        num_pool = max(10, len(self.experiments))\n",
    "        \n",
    "        with Pool(num_pool) as p:\n",
    "            self.result = p.map(f, experiments)\n",
    "            \n",
    "        #print([sum(r.rewards) for r in res])\n",
    "        #print([r.cumulative_reward for r in res])\n",
    "\"\"\"\n",
    "b = Bandit(100)\n",
    "a = PruningAgentFair(b)\n",
    "reps = RepeatedExperiment(b, a, 5)\n",
    "reps.run(10000)\n",
    "avg_rwd = np.average([r.rewards for r in reps.result], axis=0)\n",
    "plt.plot(np.convolve(avg_rwd, np.ones((100,))/100, mode='valid')) #running average: https://stackoverflow.com/a/22621523)\n",
    "plt.show()\n",
    "\"\"\"\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class UniformAgent(object):\n",
    "    def __init__(self, bandit):\n",
    "        self.bandit = bandit\n",
    "        \n",
    "        # assigns uniformly random probs. to arms, then normalize\n",
    "        self.probabilities = stats.uniform(0, 1).rvs(len(bandit.arms))\n",
    "        \n",
    "        sum_probs = sum(self.probabilities)\n",
    "        \n",
    "        # normalizes\n",
    "        self.probabilities = [p / sum_probs for p in self.probabilities]\n",
    "        \n",
    "        #checks\n",
    "        assert np.isclose(1, sum(self.probabilities))\n",
    "        \n",
    "        #saves cumulative probabilities:\n",
    "        self.cumprobs = np.cumsum(self.probabilities)\n",
    "        \n",
    "    def act(self):\n",
    "        \"\"\"\n",
    "        Selects an arm in proportion with the probabilities\n",
    "        \"\"\"\n",
    "        # code copied from OpenAI Gym gym/envs/toy_text/discrete.py\n",
    "        return (self.cumprobs > np.random.rand()).argmax()\n",
    "        \n",
    "    def argmax(self):\n",
    "        \"\"\"\n",
    "        Returns the arm with highest probability\n",
    "        \"\"\"\n",
    "        return np.argmax(self.probabilities)\n",
    "    \n",
    "    def epsilon_argmax(self, epsilon):\n",
    "        if np.random.random() < epsilon:\n",
    "            return np.random.randint(0, len(self.probabilities))  \n",
    "        return self.greedy()\n",
    "    \n",
    "# uncomment to test uniform agent\n",
    "\"\"\"\n",
    "b = Bandit(10)\n",
    "a = UniformAgent(b)\n",
    "print(a.argmax())\n",
    "plt.hist([a.act() for _ in range(10000)])\n",
    "plt.show()\n",
    "\"\"\"\n",
    "pass #prevents outputting the comment string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class UniformBiasedAgent(UniformAgent):\n",
    "    def __init__(self, bandit):\n",
    "        self.bandit = bandit\n",
    "        \n",
    "        # samples the prob of choosing the best arm from [0, 1]\n",
    "        p_best = stats.uniform(0, 1).rvs(1)[0]\n",
    "        \n",
    "        # samples the prob of choosing the other arms from [0, 1-p_best]\n",
    "        p_others = stats.uniform(0, 1 - p_best).rvs(len(bandit.arms) -1)  \n",
    "               \n",
    "        # normalizes\n",
    "        sum_others = sum(p_others)\n",
    "        #norm_factor = sum_others*(1 - p_best)\n",
    "        p_others = [p *(1 - p_best) / sum_others for p in p_others]\n",
    "        \n",
    "        #print(p_best, sum_others, sum(p_others))\n",
    "        \n",
    "        # finally assigns the probabilities\n",
    "        offset = 0 # helps on getting the prob. from correct position\n",
    "        self.probabilities = np.zeros(len(bandit.arms))\n",
    "        for i, arm in enumerate(bandit.arms):\n",
    "            if i == bandit.best_arm:\n",
    "                self.probabilities[i] = p_best\n",
    "                offset = 1 # to discount that I'm not getting p_others[i] in this iteration\n",
    "            else:\n",
    "                self.probabilities[i] = p_others[i - offset]\n",
    "        \n",
    "        #checks\n",
    "        assert np.isclose(1, sum(self.probabilities))\n",
    "        \n",
    "        #saves cumulative probabilities:\n",
    "        self.cumprobs = np.cumsum(self.probabilities)\n",
    "\n",
    "# uncomment to test uniform biased agent\n",
    "\"\"\"\n",
    "b = Bandit(10)\n",
    "a = UniformBiasedAgent(b)\n",
    "print(a.argmax())\n",
    "plt.hist([a.act() for _ in range(10000)])\n",
    "plt.show()\n",
    "\"\"\"\n",
    "pass # prevents outputting a comment string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class GaussianBiasedAgent(UniformAgent):\n",
    "    def __init__(self, bandit, mu=.5, sigma=.2):\n",
    "        self.bandit = bandit\n",
    "        \n",
    "        # samples the prob of choosing the best arm from N(mu, sigma) - ensuring it is in [0, 1]\n",
    "        p_best = stats.norm(mu, sigma).rvs(1)[0]\n",
    "        p_best = min(1, max(0, p_best))\n",
    "        \n",
    "        \n",
    "        # samples the prob of choosing the other arms from [0, 1-p_best]\n",
    "        p_others = stats.uniform(0, 1 - p_best).rvs(len(bandit.arms) -1)\n",
    "               \n",
    "        # normalizes (if needed)\n",
    "        if not np.isclose(p_best, 1):\n",
    "            sum_others = sum(p_others)\n",
    "            #norm_factor = sum_others*(1 - p_best)\n",
    "            p_others = [p *(1 - p_best) / sum_others for p in p_others]\n",
    "\n",
    "        #print(p_best, sum_others, sum(p_others))\n",
    "        \n",
    "        # finally assigns the probabilities\n",
    "        offset = 0 # helps on getting the prob. from correct position\n",
    "        self.probabilities = np.zeros(len(bandit.arms))\n",
    "        for i, arm in enumerate(bandit.arms):\n",
    "            if i == bandit.best_arm:\n",
    "                self.probabilities[i] = p_best\n",
    "                offset = 1 # to discount that I'm not getting p_others[i] in this iteration\n",
    "            else:\n",
    "                self.probabilities[i] = p_others[i - offset]\n",
    "        \n",
    "        #checks\n",
    "        #print(p_best, sum(self.probabilities))\n",
    "        assert np.isclose(1, sum(self.probabilities))\n",
    "        \n",
    "        #saves cumulative probabilities:\n",
    "        self.cumprobs = np.cumsum(self.probabilities)\n",
    "\n",
    "# uncomment to test\n",
    "\"\"\"\n",
    "b = Bandit(10)\n",
    "for i in range(1000):\n",
    "    a = GaussianBiasedAgent(b)\n",
    "#print(a.argmax())\n",
    "#plt.hist([a.act() for _ in range(10000)])\n",
    "#plt.show()\n",
    "\"\"\"\n",
    "pass # prevents outputting a comment string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "\n",
    "class PruningAgentWeak(UniformAgent):\n",
    "    \"\"\"\n",
    "    Only a few actions have nonzero probability, the best action may have zero probability\n",
    "    \"\"\"\n",
    "    def __init__(self, bandit, prune_factor=0.8):\n",
    "        self.bandit = bandit\n",
    "        \n",
    "        n_arms = len(self.bandit.arms)\n",
    "        num_nonzero = ceil(n_arms * (1 - prune_factor))\n",
    "        \n",
    "        # at least 2 actions should have non-zero probability\n",
    "        if num_nonzero < 2:\n",
    "            num_nonzero = 2\n",
    "        \n",
    "        # determines which actions will have non-zero probability\n",
    "        which_nonzero = np.random.choice(range(n_arms), num_nonzero, replace=False)\n",
    "\n",
    "        # samples the prob of choosing arms from U[0, 1]\n",
    "        probs = stats.uniform(0, 1).rvs(num_nonzero)  \n",
    "               \n",
    "        # normalizes\n",
    "        sum_probs = sum(probs)\n",
    "        probs = [p / sum_probs for p in probs]\n",
    "        \n",
    "        # determines probabilities of actions\n",
    "        self.probabilities = np.zeros(len(bandit.arms))\n",
    "        for which, prob in zip(which_nonzero, probs):\n",
    "            self.probabilities[which] = prob\n",
    "                \n",
    "        #checks\n",
    "        assert np.isclose(1, sum(self.probabilities))\n",
    "        \n",
    "        #saves cumulative probabilities:\n",
    "        self.cumprobs = np.cumsum(self.probabilities)\n",
    "\n",
    "# uncomment to test pruning agent\n",
    "\"\"\"\n",
    "b = Bandit(10)\n",
    "a = PruningAgentWeak(b, .8)\n",
    "print('b-best, a-best = %d, %d' % (b.best_arm, a.argmax()))\n",
    "plt.hist([a.act() for _ in range(10000)])\n",
    "plt.show()\n",
    "\"\"\"\n",
    "pass # prevents outputting a comment string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class PruningAgentStrong(PruningAgentWeak):\n",
    "    \"\"\"\n",
    "    Only a few actions have nonzero probability, the best action (a*) DOES NOT have zero probability\n",
    "    In fact, it is very likely that p_a* > p_a' for all a' != a*\n",
    "    \"\"\"\n",
    "    def __init__(self, bandit, prune_factor=0.8):\n",
    "        self.bandit = bandit\n",
    "        \n",
    "        n_arms = len(self.bandit.arms)\n",
    "        num_nonzero = ceil(n_arms * (1 - prune_factor))\n",
    "        \n",
    "        # at least 2 actions should have non-zero probability\n",
    "        if num_nonzero < 2:\n",
    "            num_nonzero = 2\n",
    "        \n",
    "        # determines which actions will have non-zero probability\n",
    "        which_nonzero = np.random.choice(range(n_arms), num_nonzero, replace=False)\n",
    "\n",
    "        if bandit.best_arm not in which_nonzero:\n",
    "            # replaces the first occurrence with the best\n",
    "            which_nonzero[0] = bandit.best_arm\n",
    "        else:\n",
    "            # puts best in first position by swapping\n",
    "            where = np.where(which_nonzero == bandit.best_arm) # which_nonzero.index(bandit.best_arm)\n",
    "            which_nonzero[0], which_nonzero[where] =  which_nonzero[where], which_nonzero[0]\n",
    "            \n",
    "        # samples the prob of choosing the best arm from U[0, 1]\n",
    "        # p_best > all other with high probability\n",
    "        p_best = 0 \n",
    "        while p_best <= 1.0 / num_nonzero:\n",
    "            p_best = stats.uniform(0, 1).rvs(1)[0]\n",
    "        \n",
    "        # samples the prob of choosing the other arms from U[0, 1-p_best]\n",
    "        p_others = stats.uniform(0, 1 - p_best).rvs(num_nonzero -1)  \n",
    "               \n",
    "        # normalizes\n",
    "        sum_others = sum(p_others)\n",
    "        p_others = [p *(1 - p_best) / sum_others for p in p_others]\n",
    "        \n",
    "        # construct a vector with all probabilities (best in first position)\n",
    "        p_all = [p_best] + p_others\n",
    "        \n",
    "        # determines which actions will have nonzero probability\n",
    "        self.probabilities = np.zeros(len(bandit.arms))\n",
    "        for which, prob in zip(which_nonzero, p_all):\n",
    "            self.probabilities[which] = prob\n",
    "                \n",
    "        #checks\n",
    "        assert np.isclose(1, sum(self.probabilities))\n",
    "        \n",
    "        #saves cumulative probabilities:\n",
    "        self.cumprobs = np.cumsum(self.probabilities)\n",
    "\n",
    "# uncomment to test\n",
    "\"\"\"\n",
    "b = Bandit(100)\n",
    "for _ in range(1000):\n",
    "    a = PruningAgentStrong(b, .8)\n",
    "    print('b-best, a-best = %d, %d' % (b.best_arm, a.argmax()))\n",
    "plt.hist([a.act() for _ in range(10000)])\n",
    "plt.show()\n",
    "\"\"\"\n",
    "pass # prevents outputting a comment string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class PruningAgentFair(PruningAgentWeak):\n",
    "    \"\"\"\n",
    "    Only a few actions have nonzero probability, the best action (a*) DOES NOT have zero probability\n",
    "    It is drawn from a uniform probability, just like the others :) \n",
    "    \"\"\"\n",
    "    def __init__(self, bandit, prune_factor=0.8):\n",
    "        self.bandit = bandit\n",
    "        \n",
    "        n_arms = len(self.bandit.arms)\n",
    "        num_nonzero = ceil(n_arms * (1 - prune_factor))\n",
    "        \n",
    "        # at least 2 actions should have non-zero probability\n",
    "        if num_nonzero < 2:\n",
    "            num_nonzero = 2\n",
    "        \n",
    "        # determines which actions will have non-zero probability\n",
    "        which_nonzero = np.random.choice(range(n_arms), num_nonzero, replace=False)\n",
    "\n",
    "        # ensures the best arm is among non_zero\n",
    "        if bandit.best_arm not in which_nonzero:\n",
    "            # replaces the first occurrence with the best\n",
    "            which_nonzero[0] = bandit.best_arm\n",
    "        else:\n",
    "            # puts best in first position by swapping\n",
    "            where = np.where(which_nonzero == bandit.best_arm) # which_nonzero.index(bandit.best_arm)\n",
    "            which_nonzero[0], which_nonzero[where] =  which_nonzero[where], which_nonzero[0]\n",
    "            \n",
    "        # samples the prob of choosing arms from U[0, 1]\n",
    "        probs = stats.uniform(0, 1).rvs(num_nonzero)  \n",
    "               \n",
    "        # normalizes\n",
    "        sum_probs = sum(probs)\n",
    "        probs = [p / sum_probs for p in probs]\n",
    "        \n",
    "        # determines probabilities of actions\n",
    "        self.probabilities = np.zeros(len(bandit.arms))\n",
    "        for which, prob in zip(which_nonzero, probs):\n",
    "            self.probabilities[which] = prob\n",
    "        \n",
    "        #checks\n",
    "        assert np.isclose(1, sum(self.probabilities))\n",
    "        \n",
    "        #saves cumulative probabilities:\n",
    "        self.cumprobs = np.cumsum(self.probabilities)\n",
    "\n",
    "# uncomment to test pruning agent\n",
    "\"\"\"\n",
    "b = Bandit(100)\n",
    "a = PruningAgentFair(b, .8)\n",
    "print('b-best, a-best = %d, %d' % (b.best_arm, a.argmax()))\n",
    "plt.hist([a.act() for _ in range(10000)])\n",
    "plt.show()\n",
    "\"\"\"\n",
    "pass # prevents outputting a comment string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class LearningAgent(object):\n",
    "    def __init__(self, bandit, alpha=0.1):\n",
    "        self.bandit = bandit\n",
    "        self.alpha = alpha\n",
    "        self.q = np.zeros(len(bandit.arms))\n",
    "        self.last_choice = None\n",
    "        \n",
    "    def act(self, epsilon=0.1):\n",
    "        \"\"\"\n",
    "        Epsilon greedy over actions\n",
    "        TODO: break ties arbitrarily in np.argmax\n",
    "        \"\"\"\n",
    "        if np.random.random() < epsilon:\n",
    "            self.last_choice =  np.random.randint(0, len(self.q))  \n",
    "        else:\n",
    "            self.last_choice = np.random.choice(np.flatnonzero(self.q == self.q.max())) #np.argmax(self.q)\n",
    "            #np.argmax(self.q)\n",
    "        \n",
    "        return self.last_choice\n",
    "    \n",
    "    def learn(self, choice, reward):\n",
    "        \"\"\"\n",
    "        Q-learning update rule (without states for now)\n",
    "        \"\"\"\n",
    "        self.q[choice] = self.q[choice] + self.alpha*(reward - self.q[choice])\n",
    "        \n",
    "# uncomment to test\n",
    "\"\"\"\n",
    "trials = 100000\n",
    "env = Bandit(100, arms_sigma=0)\n",
    "learner = LearningAgent(env)\n",
    "\n",
    "exp = Experiment(env, learner)\n",
    "exp.run(trials)\n",
    "    \n",
    "#stats:\n",
    "freq_best = sum([1 for x in exp.actions if x == env.best_arm])\n",
    "\n",
    "print(\"% best:\", freq_best / trials)\n",
    "#plt.hist(actions)\n",
    "#plt.hist(rewards)\n",
    "plt.plot(np.convolve(exp.rewards, np.ones((100,))/100, mode='valid')) #running average: https://stackoverflow.com/a/22621523\n",
    "plt.show()\n",
    "\"\"\"\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Controller(object):\n",
    "    def __init__(self, team, alpha=0.1):\n",
    "        \"\"\"\n",
    "        Receives the array of agents\n",
    "        \"\"\"\n",
    "        self.team = team\n",
    "        self.q = [0 for _ in self.team]\n",
    "        self.last_choice = None\n",
    "        self.last_agent_index = None\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def act(self, epsilon=0.1):\n",
    "        \"\"\"\n",
    "        Epsilon greedy over agents\n",
    "        # TODO: break ties randomly in argmax\n",
    "        \"\"\"\n",
    "        \n",
    "        if np.random.random() < epsilon:\n",
    "            self.last_agent_index =  np.random.randint(0, len(self.q))  \n",
    "        else:\n",
    "            self.last_agent_index = np.argmax(self.q)\n",
    "        \n",
    "        self.last_choice = self.team[self.last_agent_index].act()\n",
    "        \n",
    "        return self.last_choice\n",
    "    \n",
    "    def learn(self, choice, reward):\n",
    "        \"\"\"\n",
    "        Q-learning update rule (without states for now)\n",
    "        \"\"\"\n",
    "        a = self.last_agent_index\n",
    "        self.q[a] = self.q[a] + self.alpha*(reward - self.q[a])\n",
    "\n",
    "# uncomment to test\n",
    "\n",
    "trials = 10000\n",
    "env = Bandit(100, arms_sigma=0)\n",
    "weak = Controller([PruningAgentWeak(env) for _ in range(10)])\n",
    "fair = Controller([PruningAgentFair(env) for _ in range(10)])\n",
    "strong = Controller([PruningAgentStrong(env) for _ in range(10)])\n",
    "mixed = Controller([PruningAgentWeak(env) for _ in range(9)] + [PruningAgentStrong(env)])  # 9 weak + 1 strong\n",
    "\n",
    "controllers = {'weak': weak, 'fair': fair, 'strong': strong, 'mixed': mixed}\n",
    "\n",
    "for name, ctrl in controllers.items():\n",
    "    exp = Experiment(env, ctrl)\n",
    "    exp.run(trials)\n",
    "\n",
    "    #stats:\n",
    "    freq_best = sum([1 for x in exp.actions if x == env.best_arm])\n",
    "\n",
    "    print(\"%s: freq. best: %f, acc. rwd: %d\" % (name, freq_best / trials, exp.cumulative_reward) )\n",
    "    #plt.hist(actions)\n",
    "    #plt.hist(rewards)\n",
    "    plt.plot(np.convolve(exp.rewards, np.ones((100,))/100, mode='valid')) #running average: https://stackoverflow.com/a/22621523\n",
    "\n",
    "    plt.legend(controllers.keys())\n",
    "plt.show()\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# The real test\n",
    "\n",
    "team_sizes = [5, 10, 50, 100, 500]     \n",
    "bandit_sizes = [100, 1000]\n",
    "trials = 10000\n",
    "\n",
    "for n_arms in bandit_sizes:\n",
    "    bandit = Bandit(n_arms)\n",
    "    \n",
    "    learner = LearningAgent(bandit)\n",
    "    over_actions = Experiment(bandit, learner)\n",
    "    over_actions.run(trials)\n",
    "    \n",
    "    for team_sz in team_sizes:\n",
    "        ctrl_unif = Controller([UniformAgent(bandit) for _ in range(team_sz)])\n",
    "        ctrl_ubias = Controller([UniformBiasedAgent(bandit) for _ in range(team_sz)])\n",
    "        ctrl_prun = Controller([PruningAgent(bandit) for _ in range(team_sz)])\n",
    "        ctrl_gauss = Controller([GaussianBiasedAgent(bandit) for _ in range(team_sz)])\n",
    "        \n",
    "        #over_unif_agents = Experiment(bandit, ctrl_unif)\n",
    "        #over_unif_agents.run(trials)\n",
    "        \n",
    "        over_gauss_agents = Experiment(bandit, ctrl_gauss)\n",
    "        over_gauss_agents.run(trials)\n",
    "        \n",
    "        over_biased_agents = Experiment(bandit, ctrl_ubias)\n",
    "        over_biased_agents.run(trials)\n",
    "        \n",
    "        over_pruning_agents = Experiment(bandit, ctrl_prun)\n",
    "        over_pruning_agents.run(trials)\n",
    "        \n",
    "        # plots rewards over time\n",
    "        #running average from https://stackoverflow.com/a/22621523\n",
    "        plt.plot(np.convolve(over_actions.rewards, np.ones((100,))/100, mode='valid'), label='Actions') \n",
    "        plt.plot(np.convolve(over_gauss_agents.rewards, np.ones((100,))/100, mode='valid'), label='GaussAgt') \n",
    "        plt.plot(np.convolve(over_biased_agents.rewards, np.ones((100,))/100, mode='valid'), label='BiasAgt') \n",
    "        plt.plot(np.convolve(over_pruning_agents.rewards, np.ones((100,))/100, mode='valid'), label='PrunAgt') \n",
    "        plt.legend([\n",
    "            'Actions', \n",
    "            'GaussAgents', \n",
    "            'BiasedAgents', \n",
    "            'PruningAgents'\n",
    "        ])\n",
    "        plt.title(\"#actions: %d - team_size: %d\"  % (n_arms, team_sz))\n",
    "        plt.show()\n",
    "        \n",
    "        # plots freq. of best actions\n",
    "        d = {\n",
    "            'Actions': over_actions.freq_best_action, \n",
    "            'GaussAgt': over_gauss_agents.freq_best_action,\n",
    "            'BiasAgt': over_biased_agents.freq_best_action,\n",
    "            'PrunAgt': over_pruning_agents.freq_best_action,\n",
    "        }\n",
    "\n",
    "        plt.title(\"Freq. of best action, for #actions: %d - team_size: %d\"  % (n_arms, team_sz))\n",
    "        plt.bar(range(len(d)), d.values(), align='center')\n",
    "        plt.xticks(range(len(d)), d.keys(), rotation='vertical')\n",
    "        #plt.plot()\n",
    "        \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# test over team size:\n",
    "import sys\n",
    "\n",
    "n_arms = 500\n",
    "trials = 10000\n",
    "bandit = Bandit(n_arms, arms_sigma=0) # deterministic bandit\n",
    "\n",
    "# stores the difference in accumulated reward of each method vs learning over actions\n",
    "deltas = {'ubiased': [], 'gauss': [], 'prun':[]}\n",
    "rewards = {'actions':[], 'ubiased': [], 'gauss': [], 'prun':[]}\n",
    "regrets = {'actions':[], 'ubiased': [], 'gauss': [], 'prun':[]}\n",
    "x_values = []\n",
    "\n",
    "max_expected_rwd = np.max(bandit.bandits_mu) * trials\n",
    "#print(max_expected_rwd, bandit.arms[bandit.best_arm].expect())\n",
    "\n",
    "learner = LearningAgent(bandit)\n",
    "over_actions = Experiment(bandit, learner)\n",
    "over_actions.run(trials)\n",
    "\n",
    "rwd_actions = sum(over_actions.rewards)\n",
    "regret_actions = max_expected_rwd - rwd_actions\n",
    "\n",
    "team_sz = .1\n",
    "while team_sz < 1000:\n",
    "#for team_sz in range(1, 1100, 10):\n",
    "    team_sz = round(team_sz * 10)\n",
    "    sys.stdout.write(\"\\r|X| = %6d\" % team_sz)\n",
    "    \n",
    "    x_values.append(team_sz)\n",
    "    \n",
    "    ctrl_unif = Controller([UniformAgent(bandit) for _ in range(team_sz)])\n",
    "    ctrl_ubias = Controller([UniformBiasedAgent(bandit) for _ in range(team_sz)])\n",
    "    ctrl_prun = Controller([PruningAgent(bandit) for _ in range(team_sz)])\n",
    "    ctrl_gauss = Controller([GaussianBiasedAgent(bandit) for _ in range(team_sz)])\n",
    "\n",
    "    over_gauss_agents = Experiment(bandit, ctrl_gauss)\n",
    "    over_gauss_agents.run(trials)\n",
    "    rewards['gauss'].append(over_gauss_agents.cumulative_reward)\n",
    "    regrets['gauss'].append(over_gauss_agents.cumulative_regret)\n",
    "    deltas['gauss'].append(over_gauss_agents.cumulative_reward - rwd_actions)\n",
    "\n",
    "    over_biased_agents = Experiment(bandit, ctrl_ubias)\n",
    "    over_biased_agents.run(trials)\n",
    "    rewards['ubiased'].append(over_biased_agents.cumulative_reward)\n",
    "    regrets['ubiased'].append(over_biased_agents.cumulative_regret)\n",
    "    deltas['ubiased'].append(over_biased_agents.cumulative_reward - rwd_actions)\n",
    "\n",
    "    over_pruning_agents = Experiment(bandit, ctrl_prun)\n",
    "    over_pruning_agents.run(trials)\n",
    "    rewards['prun'].append(over_pruning_agents.cumulative_reward)\n",
    "    regrets['prun'].append(over_pruning_agents.cumulative_regret)\n",
    "    deltas['prun'].append(over_pruning_agents.cumulative_reward - rwd_actions)\n",
    "\n",
    "sys.stdout.write(\"\\r\" + ' ' * 70)  #clears up the output\n",
    "rewards['actions'] = np.ones(len(x_values)) * rwd_actions\n",
    "regrets['actions'] = np.ones(len(x_values)) * regret_actions\n",
    "    \n",
    "for name, y_values in rewards.items():\n",
    "    plt.plot(x_values, y_values)\n",
    "\n",
    "plt.xlabel(\"|X|\")\n",
    "plt.ylabel(\"Cumulative reward\")\n",
    "plt.legend(rewards.keys())\n",
    "plt.title(\"Cumulative rwd for |A| = %d\" % n_arms)\n",
    "plt.xscale('log')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "for name, y_values in regrets.items():\n",
    "    plt.plot(x_values, y_values)\n",
    "plt.xlabel(\"|X|\")\n",
    "plt.ylabel(\"Cumulative regret\")\n",
    "#plt.ylabel(\"R(delegate) - R(actions)\")\n",
    "    \n",
    "plt.legend(regrets.keys())\n",
    "plt.title(\"Cumulative regret for |A| = %d\" % n_arms)\n",
    "plt.show()\n",
    "\"\"\"\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test over number of actions\n",
    "import sys\n",
    "trials = 10000\n",
    "\n",
    "\n",
    "team_sizes = [10, 100, 1000]\n",
    "for team_sz in team_sizes:\n",
    "    # stores the difference in accumulated reward of each method vs learning over actions\n",
    "    deltas = {'ubiased': [], 'gauss': [], 'prun':[]}\n",
    "    rewards = {'actions':[], 'ubiased': [], 'gauss': [], 'prun':[]}\n",
    "    regrets = {'actions':[], 'ubiased': [], 'gauss': [], 'prun':[]}\n",
    "    x_values = []\n",
    "    \n",
    "    for n_arms in range(10, 500, 10):\n",
    "        sys.stdout.write(\"\\r|A|, |X| = %6d, %6d\" % (n_arms, team_sz))\n",
    "        bandit = Bandit(n_arms, arms_sigma=0) # deterministic bandit\n",
    "        learner = LearningAgent(bandit)\n",
    "        over_actions = Experiment(bandit, learner)\n",
    "        over_actions.run(trials)\n",
    "\n",
    "        rwd_actions = sum(over_actions.rewards)\n",
    "        regret_actions = max_expected_rwd - rwd_actions\n",
    "\n",
    "        x_values.append(n_arms)\n",
    "    \n",
    "        ctrl_unif = Controller([UniformAgent(bandit) for _ in range(team_sz)])\n",
    "        ctrl_ubias = Controller([UniformBiasedAgent(bandit) for _ in range(team_sz)])\n",
    "        ctrl_prun = Controller([PruningAgentFair(bandit) for _ in range(team_sz)])\n",
    "        ctrl_gauss = Controller([GaussianBiasedAgent(bandit) for _ in range(team_sz)])\n",
    "\n",
    "        over_gauss_agents = Experiment(bandit, ctrl_gauss)\n",
    "        over_gauss_agents.run(trials)\n",
    "        rewards['gauss'].append(over_gauss_agents.cumulative_reward)\n",
    "        regrets['gauss'].append(over_gauss_agents.cumulative_regret)\n",
    "        deltas['gauss'].append(over_gauss_agents.cumulative_reward - rwd_actions)\n",
    "\n",
    "        over_biased_agents = Experiment(bandit, ctrl_ubias)\n",
    "        over_biased_agents.run(trials)\n",
    "        rewards['ubiased'].append(over_biased_agents.cumulative_reward)\n",
    "        regrets['ubiased'].append(over_biased_agents.cumulative_regret)\n",
    "        deltas['ubiased'].append(over_biased_agents.cumulative_reward - rwd_actions)\n",
    "\n",
    "        over_pruning_agents = Experiment(bandit, ctrl_prun)\n",
    "        over_pruning_agents.run(trials)\n",
    "        rewards['prun'].append(over_pruning_agents.cumulative_reward)\n",
    "        regrets['prun'].append(over_pruning_agents.cumulative_regret)\n",
    "        deltas['prun'].append(over_pruning_agents.cumulative_reward - rwd_actions)\n",
    "        \n",
    "        rewards['actions'].append(over_actions.rewards)\n",
    "        regrets['actions'].append(over_actions.rewards)\n",
    "\n",
    "    sys.stdout.write(\"\\r\" + ' ' * 70)  #clears up the output\n",
    "    #print('Plotting:')\n",
    "\n",
    "    for name, y_values in rewards.items():\n",
    "        plt.plot(x_values, y_values)\n",
    "\n",
    "    plt.xlabel(\"|A|\")\n",
    "    plt.ylabel(\"Cumulative reward\")\n",
    "    plt.legend(rewards.keys())\n",
    "    plt.title(\"Cumulative rwd for |X| = %d\" % team_sz)\n",
    "    plt.show()\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# parallelism attempt #1\n",
    "import os\n",
    "from multiprocessing import Pool, Process\n",
    "\n",
    "def f(exp):\n",
    "    exp.run(10000)\n",
    "    return exp.cumulative_reward\n",
    "\n",
    "env = Bandit(100)\n",
    "#agent = Controller([PruningAgentStrong(env) for _ in range(10)])\n",
    "agent = LearningAgent(env)\n",
    "\n",
    "experiments = [Experiment(env, agent) for _ in range(10)]\n",
    "        \n",
    "#p = multiprocessing.Pool(max(10, len(self.experiments)))\n",
    "\n",
    "ps = []\n",
    "for num in range(10):\n",
    "    p = Process(target=f, args=(experiments[num], ))\n",
    "    ps.append(p)\n",
    "    p.start()\n",
    "    \n",
    "    \n",
    "for p in ps:\n",
    "    p.join()\n",
    "    \n",
    "print([e.cumulative_reward for e in experiments])\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "with Pool(5) as p:\n",
    "#    p.map(f, experiments)\n",
    "    #multiple_results = [p.apply_async(os.getpid, ()) for i in range(5)]\n",
    "    multiple_results = [p.map_async(f, experiments) for exp in experiments]\n",
    "    print([res.get(timeout=100) for res in multiple_results])\n",
    "    \n",
    "print([e.cumulative_reward for e in experiments])\n",
    "\"\"\"\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# parallelism attempt #2\n",
    "from multiprocessing import Pool\n",
    "import copy\n",
    "\n",
    "def f(exp):\n",
    "    exp.run(10000)\n",
    "    return copy.copy(exp)\n",
    "    #return exp.cumulative_reward\n",
    "\n",
    "env = Bandit(100)\n",
    "\n",
    "weak = Controller([PruningAgentWeak(env) for _ in range(10)])\n",
    "fair = Controller([PruningAgentFair(env) for _ in range(10)])\n",
    "strong = Controller([PruningAgentStrong(env) for _ in range(10)])\n",
    "mixed = Controller([PruningAgentWeak(env) for _ in range(9)] + [PruningAgentStrong(env)])  # 9 weak + 1 strong\n",
    "\n",
    "controllers = {'weak': weak, 'fair': fair, 'strong': strong, 'mixed': mixed}\n",
    "#agent = LearningAgent(env)\n",
    "experiments = [] #[Experiment(env, agent) for _ in range(10)]\n",
    "for name, ctrl in controllers.items():\n",
    "    exp = Experiment(env, ctrl)\n",
    "    experiments.append(exp)\n",
    "\n",
    "#if __name__ == '__main__':\n",
    "with Pool(4) as p:\n",
    "    res = p.map(f, experiments)\n",
    "    print([sum(r.rewards) for r in res])\n",
    "    print([r.cumulative_reward for r in res])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
